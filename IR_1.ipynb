{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'sample.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_file(file_path): \n",
    "    # Read the text file \n",
    "    with open(file_path, 'r', encoding='utf-8') as file: \n",
    "        text = file.read() \n",
    "        \n",
    "    # Tokenize the text into words \n",
    "    tokens = word_tokenize(text) \n",
    "    \n",
    "    # Convert to Lowercase \n",
    "    tokens = [word.lower() for word in tokens] \n",
    "    \n",
    "    # Remove stop words \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "     \n",
    "    # Stemming \n",
    "    stemmer = PorterStemmer() \n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens] \n",
    "    \n",
    "    # Join the processed tokens back into a string \n",
    "    processed_text = ' '.join(stemmed_tokens) \n",
    "    \n",
    "    return processed_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text:\n",
      " past decad advanc technolog transform busi oper commun custom artifici intellig machin learn compani use sophist tool gain insight custom behavior predict trend person servic instanc platform reli heavili recommend algorithm suggest product base past purchas brows histori even time day approach boost sale also improv user experi offer tailor suggest addit social media platform becom essenti brand engag allow busi reach global audienc instantli recent report reveal 80 consum expect brand respond social media inquiri within 24 hour yet relianc digit tool grow concern data privaci vast amount data collect pose risk grow need stringent regul protect user privaci compani must ensur complianc law gdpr europ ccpa california fail lead signific financi penalti reput damag despit challeng technolog continu drive innov develop area like blockchain promis increas transpar secur transact organ navig chang balanc technolog effici ethic respons play defin role success reput digit age\n"
     ]
    }
   ],
   "source": [
    "processed_text = preprocess_text_file(file_path) \n",
    "print(\"Processed Text:\\n\", processed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
